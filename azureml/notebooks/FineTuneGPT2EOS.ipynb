{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293cb19b",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60201782",
   "metadata": {},
   "source": [
    "In order to get started, we will install the libraries in `requirements.txt` that we will use to load any pretrained huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf46170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcc4f5",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6648b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Parameters\n",
    "preprocessing_num_workers = None #The number of processes to use for the preprocessing.\n",
    "overwrite_cache = True # Overwrite the cached training and evaluation sets.\n",
    "\n",
    "# Training Parameters\n",
    "max_train_samples = None #For debugging purposes or quicker training, truncate the number of training examples to this value if set.\n",
    "max_eval_samples = None #For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.\n",
    "model_name = \"gpt2\"\n",
    "output_dir = \"outputEOS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bddead",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6ee8a",
   "metadata": {},
   "source": [
    "We will use a small dataset for testing purposes. \n",
    "\n",
    "Dataset `banking77` composed of online banking queries annotated with their corresponding intents.\n",
    "\n",
    "`banking77` dataset provides a very fine-grained set of intents in a banking domain. It comprises 13,083 customer service queries labeled with 77 intents. \n",
    "\n",
    "For our purpose, we will ignore the intent label and focus on generating texts from the banking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a094948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/azureuser/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "raw_datasets = load_dataset(\"banking77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2094d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('text', [\"I need help- I got mugged yesterday and they took everything so I can't use the app.\"]), ('label', [42])])\n",
      "OrderedDict([('text', ['My phone was stolen yesterday.  What should I do?']), ('label', [42])])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.sample(range(len(raw_datasets[\"train\"])), 1)\n",
    "print(raw_datasets[\"train\"][index])\n",
    "\n",
    "index = random.sample(range(len(raw_datasets[\"test\"])), 1)\n",
    "print(raw_datasets[\"test\"][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc78d56",
   "metadata": {},
   "source": [
    "# Preprocess & Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad24d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_column_name = \"text\"\n",
    "column_names = raw_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488cb63",
   "metadata": {},
   "source": [
    "## Preprocess Dataset & add eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e3cdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929dc0b72fd14bc9a3fb758cbb41b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding eos_token to each example in the dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b6c8119a8a443cbbbb1be2ef52fafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding eos_token to each example in the dataset:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main data processing function that will add eos_token to each text in the dataset\n",
    "def add_eos_token(examples):\n",
    "    examples_with_eos = examples\n",
    "    examples_with_eos[text_column_name] = [x + tokenizer.eos_token for x in examples[text_column_name]]  \n",
    "    return examples_with_eos\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    add_eos_token,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=f\"Adding eos_token to each example in the dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ae0c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('label', [34]), ('text', ['There is an extra 1Â£ charge on my app. Why did you charge me extra?<|endoftext|>'])])\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(range(len(raw_datasets[\"train\"])), 1)\n",
    "\n",
    "print(raw_datasets[\"train\"][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3373322",
   "metadata": {},
   "source": [
    "## Tokenize dataset using gpt2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c01b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd822ddbe15410c847059508e706785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7b39d0d8104a21ad6d26b04d452856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe1a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('label', [20]), ('text', [\"I didn't withdraw the amount of cash that is showing up in the app.<|endoftext|>\"])])\n",
      "OrderedDict([('attention_mask', [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), ('input_ids', [[40, 1422, 470, 8399, 262, 2033, 286, 5003, 326, 318, 4478, 510, 287, 262, 598, 13, 50256]])])\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(range(len(raw_datasets[\"train\"])), 1)\n",
    "\n",
    "print(raw_datasets[\"train\"][index])\n",
    "print(tokenized_datasets[\"train\"][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68122bce",
   "metadata": {},
   "source": [
    "# Concatenate all texts from our dataset and generate chunks of block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227beb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b225731516da4d53abefcfaf0662051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c31008de434111b102219d1146a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    # The tokenizer picked seems to have a very large `model_max_length`\n",
    "    block_size = 1024\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=len(tokenized_datasets[\"train\"]), # if training size is very small, like in our case.\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ddcfaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 11, 'text': 'I am still waiting on my card?<|endoftext|>'}\n",
      "{'label': 11, 'text': \"What can I do if my card still hasn't arrived after 2 weeks?<|endoftext|>\"}\n",
      "{'label': 11, 'text': 'I have been waiting over a week. Is the card still coming?<|endoftext|>'}\n",
      "{'label': 11, 'text': 'Can I track my card while it is in the process of delivery?<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0])\n",
    "print(raw_datasets[\"train\"][1])\n",
    "print(raw_datasets[\"train\"][2])\n",
    "print(raw_datasets[\"train\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4594b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [40, 716, 991, 4953, 319, 616, 2657, 30, 50256]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2061, 460, 314, 466, 611, 616, 2657, 991, 5818, 470, 5284, 706, 362, 2745, 30, 50256]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [40, 423, 587, 4953, 625, 257, 1285, 13, 1148, 262, 2657, 991, 2406, 30, 50256]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [6090, 314, 2610, 616, 2657, 981, 340, 318, 287, 262, 1429, 286, 7585, 30, 50256]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(tokenized_datasets[\"train\"][1])\n",
    "print(tokenized_datasets[\"train\"][2])\n",
    "print(tokenized_datasets[\"train\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e12557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 991, 4953, 319, 616, 2657, 30, 50256, 2061, 460, 314, 466, 611, 616, 2657, 991, 5818, 470, 5284, 706, 362, 2745, 30, 50256, 40, 423, 587, 4953, 625, 257, 1285, 13, 1148, 262, 2657, 991, 2406, 30, 50256]\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"][0]['input_ids'][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a043ca8",
   "metadata": {},
   "source": [
    "# Prepare Training & Evaluation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0611f",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Recheck script train/eval datasets! It seems training data is split even if test set is provided!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946213e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30fd70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_train_samples is not None:\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))\n",
    "if max_eval_samples is not None:\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915f060",
   "metadata": {},
   "source": [
    "# Set Logging Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924de298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "#for index in random.sample(range(len(train_dataset)), 3):\n",
    "    #logger.info(f\"Sample {index} of the training set: {train_dataset[index]}. \\n\")\n",
    "    #logger.info(f\"Sample {index} of the training set shape: {len(train_dataset[index]['input_ids'])}. \\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7170257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:37:54 DEBUG:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "06:37:54 DEBUG:Creating converter from 7 to 5\n",
      "06:37:54 DEBUG:Creating converter from 5 to 7\n",
      "06:37:54 DEBUG:Creating converter from 7 to 5\n",
      "06:37:54 DEBUG:Creating converter from 5 to 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': <tf.Tensor: shape=(1, 1024), dtype=int64, numpy=array([[1, 1, 1, ..., 1, 1, 1]])>, 'input_ids': <tf.Tensor: shape=(1, 1024), dtype=int64, numpy=array([[50256,  5195,  2125, ...,  3342,    13, 50256]])>, 'labels': <tf.Tensor: shape=(1, 1024), dtype=int64, numpy=array([[50256,  5195,  2125, ...,  3342,    13, 50256]])>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "index = random.sample(range(len(train_dataset)), 1)\n",
    "example = train_dataset[index]\n",
    "example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int64) for key, arr in example.items()}\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea274662",
   "metadata": {},
   "source": [
    "# Check Training Parameters\n",
    "\n",
    "We can customize the training arguments using training_args if we want, or hypertune some on a seperate validation set (might take a huge amount of time though).\n",
    "\n",
    "For more arguments, check: https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc25632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:37:56 INFO:PyTorch: setting up devices\n",
      "06:37:56 INFO:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "06:37:56 INFO:Tensorflow: setting up strategy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'init_lr': 5e-05,\n",
       " 'num_replicas': 1,\n",
       " 'strategy': <tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy at 0x7fab5d626518>,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'batches_per_epoch': 18,\n",
       " 'num_train_steps': 54,\n",
       " 'num_warmup_steps': 0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'weight_decay_rate': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(output_dir=output_dir)\n",
    "\n",
    "num_replicas = training_args.strategy.num_replicas_in_sync\n",
    "batches_per_epoch = len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size)\n",
    "\n",
    "{\n",
    "    \"init_lr\": training_args.learning_rate,\n",
    "    \"num_replicas\": num_replicas,\n",
    "    \"strategy\": training_args.strategy,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"batches_per_epoch\": len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size),\n",
    "    \"num_train_steps\": int(training_args.num_train_epochs * batches_per_epoch),\n",
    "    \"num_warmup_steps\": training_args.warmup_steps,\n",
    "    \"adam_beta1\": training_args.adam_beta1,\n",
    "    \"adam_beta2\": training_args.adam_beta2,\n",
    "    \"adam_epsilon\": training_args.adam_epsilon,\n",
    "    \"weight_decay_rate\": training_args.weight_decay\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891dbcf",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Load Pretrained Model \n",
    "* Resize the number of token embeddings in the model to that of the tokenizer\n",
    "    * Since our model and tokenizer belong to the same model, the number of token embeddings should be the same.\n",
    "    \n",
    "* Generate tf.data.Dataset (s) Sample Generator:\n",
    "    * Reoreder batch randomly.\n",
    "    * Convert each tokenized text to a tensor.\n",
    " \n",
    "* Define a callback SavePretrainedCallback that will save the model checkpoint at the end of each epoch.\n",
    "\n",
    "* Define the neural network optimizer from the arguments set in the training_args!\n",
    "\n",
    "* Define the loss: We are using a dummy loss that will minimize the difference between predicted and real next token.\n",
    "    * There should be a smarter loss.\n",
    "\n",
    "* Fit the model over the training dataset & evaluate the model over the eval dataset.\n",
    "\n",
    "* Log the loss & the perplexity metric of the model.\n",
    "\n",
    "* Save the final model to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81c379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:37:56 INFO:PyTorch: setting up devices\n",
      "06:37:56 INFO:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "06:37:56 INFO:Tensorflow: setting up strategy\n",
      "06:37:57 INFO:loading weights file https://huggingface.co/gpt2/resolve/main/tf_model.h5 from cache at /home/azureuser/.cache/huggingface/transformers/4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5\n",
      "06:38:00 WARNING:All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "06:38:00 WARNING:All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "06:38:00 INFO:***** Running training *****\n",
      "06:38:00 INFO:  Num examples = 144\n",
      "06:38:00 INFO:  Num Epochs = 3.0\n",
      "06:38:00 INFO:  Instantaneous batch size per device = 8\n",
      "06:38:00 INFO:  Total train batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:38:03 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).06:38:04 WARNING:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fac5c76b528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fac7a26ad90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fac5c76b528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fac7a26ad90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "06:38:05 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "06:38:05 WARNING:From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "06:38:13 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "06:38:13 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 2.4651 - loss_loss: 2.4651 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:48:27 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "06:48:27 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 782s 43s/step - loss: 2.4651 - loss_loss: 2.4651 - val_loss: 2.1969 - val_loss_loss: 2.1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:51:02 DEBUG:Creating converter from 5 to 3\n",
      "06:51:05 INFO:Model weights saved in outputEOS/tf_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "18/18 [==============================] - 781s 44s/step - loss: 2.1733 - loss_loss: 2.1733 - val_loss: 2.1195 - val_loss_loss: 2.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:04:10 INFO:Model weights saved in outputEOS/tf_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "18/18 [==============================] - 791s 44s/step - loss: 2.1180 - loss_loss: 2.1180 - val_loss: 2.1040 - val_loss_loss: 2.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17:25 INFO:Model weights saved in outputEOS/tf_model.h5\n",
      "07:17:25 INFO:  Final train loss: 2.118\n",
      "07:17:25 INFO:  Final train perplexity: 8.314\n",
      "07:17:25 INFO:  Final validation loss: 2.104\n",
      "07:17:25 INFO:  Final validation perplexity: 8.199\n",
      "07:17:28 INFO:Model weights saved in outputEOS/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from functools import partial\n",
    "from transformers import AutoConfig, TFAutoModelForCausalLM\n",
    "from transformers import create_optimizer\n",
    "\n",
    "def sample_generator(dataset, tokenizer):\n",
    "    # Trim off the last partial batch if present\n",
    "    sample_ordering = np.random.permutation(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int64) for key, arr in example.items()}\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper classes\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
    "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
    "    # that saves the model with this method after each epoch.\n",
    "    def __init__(self, output_dir, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "\n",
    "training_args = TFTrainingArguments(output_dir=output_dir)\n",
    "#training_args.per_device_train_batch_size = 32\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    num_replicas = training_args.strategy.num_replicas_in_sync\n",
    "\n",
    "    # region TF Dataset preparation\n",
    "    train_generator = partial(sample_generator, train_dataset, tokenizer)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "        for feature in train_dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    train_sig = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_train_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_sig)\n",
    "        .with_options(options)\n",
    "        .batch(batch_size=num_replicas * training_args.per_device_train_batch_size, drop_remainder=True)\n",
    "        .repeat(int(training_args.num_train_epochs))\n",
    "    )\n",
    "    eval_generator = partial(sample_generator, eval_dataset, tokenizer)\n",
    "    eval_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "        for feature in eval_dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    eval_sig = (eval_signature, eval_signature[\"labels\"])\n",
    "    tf_eval_dataset = (\n",
    "        tf.data.Dataset.from_generator(eval_generator, output_signature=eval_sig)\n",
    "        .with_options(options)\n",
    "        .batch(batch_size=num_replicas * training_args.per_device_eval_batch_size, drop_remainder=True)\n",
    "        .repeat(int(training_args.num_train_epochs))\n",
    "    )\n",
    "    # endregion\n",
    "    # region Optimizer and loss\n",
    "    \n",
    "    batches_per_epoch = len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size)\n",
    "    # Bias and layernorm weights are automatically excluded from the decay\n",
    "    optimizer, lr_schedule = create_optimizer(\n",
    "        init_lr=training_args.learning_rate,\n",
    "        num_train_steps=int(training_args.num_train_epochs * batches_per_epoch),\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        adam_beta1=training_args.adam_beta1,\n",
    "        adam_beta2=training_args.adam_beta2,\n",
    "        adam_epsilon=training_args.adam_epsilon,\n",
    "        weight_decay_rate=training_args.weight_decay,\n",
    "    )\n",
    "\n",
    "    def dummy_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(y_pred)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss={\"loss\": dummy_loss})\n",
    "    # endregion\n",
    "\n",
    "    # region Training and validation\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size = {training_args.per_device_train_batch_size * num_replicas}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        tf_train_dataset,\n",
    "        validation_data=tf_eval_dataset,\n",
    "        epochs=int(training_args.num_train_epochs),\n",
    "        steps_per_epoch=len(train_dataset) // (training_args.per_device_train_batch_size * num_replicas),\n",
    "        callbacks=[SavePretrainedCallback(output_dir=training_args.output_dir)],\n",
    "    )\n",
    "    try:\n",
    "        train_perplexity = math.exp(history.history[\"loss\"][-1])\n",
    "    except OverflowError:\n",
    "        train_perplexity = math.inf\n",
    "    try:\n",
    "        validation_perplexity = math.exp(history.history[\"val_loss\"][-1])\n",
    "    except OverflowError:\n",
    "        validation_perplexity = math.inf\n",
    "    logger.info(f\"  Final train loss: {history.history['loss'][-1]:.3f}\")\n",
    "    logger.info(f\"  Final train perplexity: {train_perplexity:.3f}\")\n",
    "    logger.info(f\"  Final validation loss: {history.history['val_loss'][-1]:.3f}\")\n",
    "    logger.info(f\"  Final validation perplexity: {validation_perplexity:.3f}\")\n",
    "    # endregion\n",
    "\n",
    "    if training_args.output_dir is not None:\n",
    "        model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fcc9cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17:28 INFO:  Final train loss: 2.118\n",
      "07:17:28 INFO:  Final train perplexity: 8.314\n",
      "07:17:28 INFO:  Final validation loss: 2.104\n",
      "07:17:28 INFO:  Final validation perplexity: 8.199\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "try:\n",
    "    train_perplexity = math.exp(history.history[\"loss\"][-1])\n",
    "except OverflowError:\n",
    "    train_perplexity = math.inf\n",
    "try:\n",
    "    validation_perplexity = math.exp(history.history[\"val_loss\"][-1])\n",
    "except OverflowError:\n",
    "    validation_perplexity = math.inf\n",
    "logger.info(f\"  Final train loss: {history.history['loss'][-1]:.3f}\")\n",
    "logger.info(f\"  Final train perplexity: {train_perplexity:.3f}\")\n",
    "logger.info(f\"  Final validation loss: {history.history['val_loss'][-1]:.3f}\")\n",
    "logger.info(f\"  Final validation perplexity: {validation_perplexity:.3f}\")\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a61992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17:32 INFO:Model weights saved in outputEOS/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "if training_args.output_dir is not None:\n",
    "    model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188918fa",
   "metadata": {},
   "source": [
    "# Use Fine-tuned Model\n",
    "\n",
    "Now that we have trained our new language model on new data, lets give it a try! We will want to use the path to the directory that the script outputs the model file to, and load it up to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d315046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17:33 INFO:loading weights file outputEOS/tf_model.h5\n",
      "07:17:34 DEBUG:Creating converter from 3 to 5\n",
      "07:17:51 WARNING:All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "07:17:51 WARNING:All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at outputEOS.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# setup imports to use the model\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c341f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:34:30 WARNING:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Hello\n",
      "\n",
      "2: Hello the data on the card is incorrect. There were 1 items missing from my account.\n",
      "\n",
      "3: Hello that's what I thought.\n",
      "\n",
      "4: Hello for my help for my help when I need help.\n",
      "\n",
      "5: Hello for this. Would I do it if you asked? Well not if that would get much better. Have a look. I am not sure what\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello\", return_tensors='tf')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length=30,  \n",
    "    num_return_sequences=5,\n",
    "    #no_repeat_ngram_size=2,\n",
    "    #repetition_penalty=1.5,\n",
    "    #top_p=0.92,\n",
    "    #temperature=.85,\n",
    "    do_sample=True,\n",
    "    #top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i + 1,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1594358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:34:50 WARNING:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Hello from India. I've arrived in India, with your visa in the process as well for some time. If possible, please let me know what\n",
      "\n",
      "2: Hello a long time I bought it. It's the best value that I could buy yet.\n",
      "\n",
      "3: Hello: I saw that in your account, did you send me an ex check with this? I don't recall you sending me a check.\n",
      "\n",
      "4: Hello: Can you tell me what your opinion about The Witcher 2's character design, especially in terms of character models?\n",
      "\n",
      "I think it's\n",
      "\n",
      "5: Hello?\n",
      "\n",
      "No one checked it out yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello\", return_tensors='tf')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length=30,  \n",
    "    num_return_sequences=5,\n",
    "    #no_repeat_ngram_size=2,\n",
    "    #repetition_penalty=1.5,\n",
    "    #top_p=0.92,\n",
    "    #temperature=.85,\n",
    "    do_sample=True,\n",
    "    #top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i + 1,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a74ab9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = {}\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "    json_output[i+1] = tokenizer.decode(beam, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "659eb4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: \"Hello from India. I've arrived in India, with your visa in the process as well for some time. If possible, please let me know what\",\n",
       " 2: \"Hello a long time I bought it. It's the best value that I could buy yet.\",\n",
       " 3: \"Hello: I saw that in your account, did you send me an ex check with this? I don't recall you sending me a check.\",\n",
       " 4: \"Hello: Can you tell me what your opinion about The Witcher 2's character design, especially in terms of character models?\\n\\nI think it's\",\n",
       " 5: 'Hello?\\n\\nNo one checked it out yet.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
