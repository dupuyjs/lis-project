{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81982f7a",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf05c1",
   "metadata": {},
   "source": [
    "In order to get started, we will install the libraries in `requirements.txt` that we will use to load any pretrained huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecc9d2",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98514da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Parameters\n",
    "preprocessing_num_workers = None # The number of processes to use for the preprocessing.\n",
    "overwrite_cache = True # Overwrite the cached training and evaluation sets.\n",
    "\n",
    "# Training Parameters\n",
    "max_train_samples = None # For debugging purposes or quicker training, truncate the number of training examples to this value if set.\n",
    "max_eval_samples = None # For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.\n",
    "model_name = \"gpt2\"\n",
    "output_dir = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6354450",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "We will use a small dataset for testing purposes. \n",
    "\n",
    "Dataset `banking77` composed of online banking queries annotated with their corresponding intents.\n",
    "\n",
    "`banking77` dataset provides a very fine-grained set of intents in a banking domain. It comprises 13,083 customer service queries labeled with 77 intents. \n",
    "\n",
    "For our purpose, we will ignore the intent label and focus on generating texts from the banking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722f1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/azureuser/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "raw_datasets = load_dataset(\"banking77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e99258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3080\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa639491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('text', ['For the disposable cards, what are the restrictions?']), ('label', [29])])\n",
      "OrderedDict([('text', ['How can I transfer money to my account?']), ('label', [65])])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.sample(range(len(raw_datasets[\"train\"])), 1)\n",
    "print(raw_datasets[\"train\"][index])\n",
    "\n",
    "index = random.sample(range(len(raw_datasets[\"test\"])), 1)\n",
    "print(raw_datasets[\"test\"][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59714c3",
   "metadata": {},
   "source": [
    "# Tokenize dataset using gpt2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3a2b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c960f41179b45bdbebb790343d0f849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd1fec01ba94a8dbde5997dd64339b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_column_name = \"text\"\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "            \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df85d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('text', ['how to get new card after atm eats it']), ('label', [18])])\n",
      "OrderedDict([('attention_mask', [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), ('input_ids', [[4919, 284, 651, 649, 2657, 706, 379, 76, 25365, 340]])])\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(range(len(raw_datasets[\"train\"])), 1)\n",
    "\n",
    "print(raw_datasets[\"train\"][index])\n",
    "print(tokenized_datasets[\"train\"][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d96c3",
   "metadata": {},
   "source": [
    "# Concatenate all texts from our dataset and generate chunks of block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddfd082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83d9a41af5438fa5e5738f4f712c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73b917518bc4b77b52ba09739b57920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    # The tokenizer picked seems to have a very large `model_max_length`\n",
    "    block_size = 1024\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=len(tokenized_datasets[\"train\"]), # if training size is very small, like in our case.\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa252bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I am still waiting on my card?', 'label': 11}\n",
      "{'text': \"What can I do if my card still hasn't arrived after 2 weeks?\", 'label': 11}\n",
      "{'text': 'I have been waiting over a week. Is the card still coming?', 'label': 11}\n",
      "{'text': 'Can I track my card while it is in the process of delivery?', 'label': 11}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0])\n",
    "print(raw_datasets[\"train\"][1])\n",
    "print(raw_datasets[\"train\"][2])\n",
    "print(raw_datasets[\"train\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3018380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [40, 716, 991, 4953, 319, 616, 2657, 30]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2061, 460, 314, 466, 611, 616, 2657, 991, 5818, 470, 5284, 706, 362, 2745, 30]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [40, 423, 587, 4953, 625, 257, 1285, 13, 1148, 262, 2657, 991, 2406, 30]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [6090, 314, 2610, 616, 2657, 981, 340, 318, 287, 262, 1429, 286, 7585, 30]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(tokenized_datasets[\"train\"][1])\n",
    "print(tokenized_datasets[\"train\"][2])\n",
    "print(tokenized_datasets[\"train\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "070fb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 716, 991, 4953, 319, 616, 2657, 30, 2061, 460, 314, 466, 611, 616, 2657, 991, 5818, 470, 5284, 706, 362, 2745, 30, 40, 423, 587, 4953, 625, 257, 1285, 13, 1148, 262, 2657, 991, 2406, 30, 6090, 314, 2610]\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets[\"train\"][0]['input_ids'][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d59899",
   "metadata": {},
   "source": [
    "If we want each line to be treated seperately. So we instead pad each line or truncate each line to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "370f05ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindex = random.sample(range(len(raw_datasets)), 1)\\n\\nprint(raw_datasets[\"train\"][index])\\nprint(tokenized_datasets[\"train\"][index])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_column_name = \"text\"\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "padding = \"max_length\"\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name],\n",
    "                     padding=padding,\n",
    "                     truncation=True,\n",
    "                     max_length=max_seq_length)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "index = random.sample(range(len(raw_datasets)), 1)\n",
    "\n",
    "print(raw_datasets[\"train\"][index])\n",
    "print(tokenized_datasets[\"train\"][index])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "473cdd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef add_label(examples):\\n    examples[\"labels\"] = examples[\"input_ids\"].copy()\\n    return examples\\n\\nlm_datasets = tokenized_datasets.map(\\n    add_label,\\n    batched=True,\\n    num_proc=preprocessing_num_workers,\\n    load_from_cache_file=not overwrite_cache,\\n    desc=f\"Adding label to each text\",\\n)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def add_label(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    add_label,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    desc=f\"Adding label to each text\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce5f12",
   "metadata": {},
   "source": [
    "# Prepare Training & Evaluation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a328270",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Recheck script train/eval datasets! It seems training data is split even if test set is provided!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef3f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05c8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_train_samples is not None:\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))\n",
    "if max_eval_samples is not None:\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f319c60",
   "metadata": {},
   "source": [
    "# Set Logging Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ea0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "#for index in random.sample(range(len(train_dataset)), 3):\n",
    "    #logger.info(f\"Sample {index} of the training set: {train_dataset[index]}. \\n\")\n",
    "    #logger.info(f\"Sample {index} of the training set shape: {len(train_dataset[index]['input_ids'])}. \\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92081ada",
   "metadata": {},
   "source": [
    "# Check Trainig Parameters\n",
    "\n",
    "We can customize the training arguments using training_args if we want, or hypertune some on a seperate validation set (might take a huge amount of time though).\n",
    "\n",
    "For more arguments, check: https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e78a4ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:42:18 DEBUG:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "05:42:19 DEBUG:Creating converter from 7 to 5\n",
      "05:42:19 DEBUG:Creating converter from 5 to 7\n",
      "05:42:19 DEBUG:Creating converter from 7 to 5\n",
      "05:42:19 DEBUG:Creating converter from 5 to 7\n",
      "05:42:19 INFO:PyTorch: setting up devices\n",
      "05:42:19 INFO:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "05:42:19 INFO:Tensorflow: setting up strategy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'init_lr': 5e-05,\n",
       " 'num_replicas': 1,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'batches_per_epoch': 16,\n",
       " 'num_train_steps': 48,\n",
       " 'num_warmup_steps': 0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'weight_decay_rate': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(output_dir=output_dir)\n",
    "\n",
    "num_replicas = training_args.strategy.num_replicas_in_sync\n",
    "batches_per_epoch = len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size)\n",
    "\n",
    "{\n",
    "    \"init_lr\": training_args.learning_rate,\n",
    "    \"num_replicas\": num_replicas,\n",
    "    \"num_train_epochs\": training_args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"batches_per_epoch\": len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size),\n",
    "    \"num_train_steps\": int(training_args.num_train_epochs * batches_per_epoch),\n",
    "    \"num_warmup_steps\": training_args.warmup_steps,\n",
    "    \"adam_beta1\": training_args.adam_beta1,\n",
    "    \"adam_beta2\": training_args.adam_beta2,\n",
    "    \"adam_epsilon\": training_args.adam_epsilon,\n",
    "    \"weight_decay_rate\": training_args.weight_decay\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57a555",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0b148",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Load Pretrained Model \n",
    "* Resize the number of token embeddings in the model to that of the tokenizer\n",
    "    * Since our model and tokenizer belong to the same model, the number of token embeddings should be the same.\n",
    "    \n",
    "* Generate tf.data.Dataset (s) Sample Generator:\n",
    "    * Reoreder batch randomly.\n",
    "    * Convert each tokenized text to a tensor.\n",
    " \n",
    "* Define a callback SavePretrainedCallback that will save the model checkpoint at the end of each epoch.\n",
    "\n",
    "* Define the neural network optimizer from the arguments set in the training_args!\n",
    "\n",
    "* Define the loss: We are using a dummy loss that will minimize the difference between predicted and real next token.\n",
    "    * There should be a smarter loss.\n",
    "\n",
    "* Fit the model over the training dataset & evaluate the model over the eval dataset.\n",
    "\n",
    "* Log the loss & the perplexity metric of the model.\n",
    "\n",
    "* Save the final model to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d294b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:42:20 INFO:PyTorch: setting up devices\n",
      "05:42:20 INFO:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "05:42:20 INFO:Tensorflow: setting up strategy\n",
      "05:42:21 INFO:loading weights file https://huggingface.co/gpt2/resolve/main/tf_model.h5 from cache at /home/azureuser/.cache/huggingface/transformers/4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5\n",
      "05:42:23 WARNING:All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "05:42:23 WARNING:All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "05:42:23 INFO:***** Running training *****\n",
      "05:42:23 INFO:  Num examples = 134\n",
      "05:42:23 INFO:  Num Epochs = 3.0\n",
      "05:42:23 INFO:  Instantaneous batch size per device = 8\n",
      "05:42:23 INFO:  Total train batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:42:26 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).05:42:28 WARNING:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd1735b9528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fd18d0f1d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd1735b9528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fd18d0f1d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "05:42:28 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "05:42:29 WARNING:From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "05:42:36 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "05:42:36 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 2.4532 - loss_loss: 2.4532 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:51:29 WARNING:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "05:51:29 WARNING:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 669s 41s/step - loss: 2.4532 - loss_loss: 2.4532 - val_loss: 2.3416 - val_loss_loss: 2.3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:53:32 DEBUG:Creating converter from 5 to 3\n",
      "05:53:36 INFO:Model weights saved in output/tf_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "16/16 [==============================] - 676s 43s/step - loss: 2.2859 - loss_loss: 2.2859 - val_loss: 2.2824 - val_loss_loss: 2.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:04:56 INFO:Model weights saved in output/tf_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "16/16 [==============================] - 671s 42s/step - loss: 2.2322 - loss_loss: 2.2322 - val_loss: 2.2549 - val_loss_loss: 2.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:16:11 INFO:Model weights saved in output/tf_model.h5\n",
      "06:16:11 INFO:  Final train loss: 2.232\n",
      "06:16:11 INFO:  Final train perplexity: 9.320\n",
      "06:16:11 INFO:  Final validation loss: 2.255\n",
      "06:16:11 INFO:  Final validation perplexity: 9.534\n",
      "06:16:15 INFO:Model weights saved in output/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from functools import partial\n",
    "from transformers import AutoConfig, TFAutoModelForCausalLM\n",
    "from transformers import create_optimizer\n",
    "\n",
    "def sample_generator(dataset, tokenizer):\n",
    "    # Trim off the last partial batch if present\n",
    "    sample_ordering = np.random.permutation(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int64) for key, arr in example.items()}\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper classes\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
    "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
    "    # that saves the model with this method after each epoch.\n",
    "    def __init__(self, output_dir, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "\n",
    "training_args = TFTrainingArguments(output_dir=output_dir)\n",
    "#training_args.per_device_train_batch_size = 32\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    num_replicas = training_args.strategy.num_replicas_in_sync\n",
    "\n",
    "    # region TF Dataset preparation\n",
    "    train_generator = partial(sample_generator, train_dataset, tokenizer)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "        for feature in train_dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    train_sig = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_train_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_sig)\n",
    "        .with_options(options)\n",
    "        .batch(batch_size=num_replicas * training_args.per_device_train_batch_size, drop_remainder=True)\n",
    "        .repeat(int(training_args.num_train_epochs))\n",
    "    )\n",
    "    eval_generator = partial(sample_generator, eval_dataset, tokenizer)\n",
    "    eval_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int64)\n",
    "        for feature in eval_dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    eval_sig = (eval_signature, eval_signature[\"labels\"])\n",
    "    tf_eval_dataset = (\n",
    "        tf.data.Dataset.from_generator(eval_generator, output_signature=eval_sig)\n",
    "        .with_options(options)\n",
    "        .batch(batch_size=num_replicas * training_args.per_device_eval_batch_size, drop_remainder=True)\n",
    "        .repeat(int(training_args.num_train_epochs))\n",
    "    )\n",
    "    # endregion\n",
    "    # region Optimizer and loss\n",
    "    \n",
    "    batches_per_epoch = len(train_dataset) // (num_replicas * training_args.per_device_train_batch_size)\n",
    "    # Bias and layernorm weights are automatically excluded from the decay\n",
    "    optimizer, lr_schedule = create_optimizer(\n",
    "        init_lr=training_args.learning_rate,\n",
    "        num_train_steps=int(training_args.num_train_epochs * batches_per_epoch),\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        adam_beta1=training_args.adam_beta1,\n",
    "        adam_beta2=training_args.adam_beta2,\n",
    "        adam_epsilon=training_args.adam_epsilon,\n",
    "        weight_decay_rate=training_args.weight_decay,\n",
    "    )\n",
    "\n",
    "    def dummy_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(y_pred)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss={\"loss\": dummy_loss})\n",
    "    # endregion\n",
    "\n",
    "    # region Training and validation\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size = {training_args.per_device_train_batch_size * num_replicas}\")\n",
    "\n",
    "    history = model.fit(\n",
    "        tf_train_dataset,\n",
    "        validation_data=tf_eval_dataset,\n",
    "        epochs=int(training_args.num_train_epochs),\n",
    "        steps_per_epoch=len(train_dataset) // (training_args.per_device_train_batch_size * num_replicas),\n",
    "        callbacks=[SavePretrainedCallback(output_dir=training_args.output_dir)],\n",
    "    )\n",
    "    try:\n",
    "        train_perplexity = math.exp(history.history[\"loss\"][-1])\n",
    "    except OverflowError:\n",
    "        train_perplexity = math.inf\n",
    "    try:\n",
    "        validation_perplexity = math.exp(history.history[\"val_loss\"][-1])\n",
    "    except OverflowError:\n",
    "        validation_perplexity = math.inf\n",
    "    logger.info(f\"  Final train loss: {history.history['loss'][-1]:.3f}\")\n",
    "    logger.info(f\"  Final train perplexity: {train_perplexity:.3f}\")\n",
    "    logger.info(f\"  Final validation loss: {history.history['val_loss'][-1]:.3f}\")\n",
    "    logger.info(f\"  Final validation perplexity: {validation_perplexity:.3f}\")\n",
    "    # endregion\n",
    "\n",
    "    if training_args.output_dir is not None:\n",
    "        model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ca1bb",
   "metadata": {},
   "source": [
    "# Use Fine-tuned Model\n",
    "\n",
    "Now that we have trained our new language model on new data, lets give it a try! We will want to use the path to the directory that the script outputs the model file to, and load it up to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e05cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:16:15 INFO:loading weights file output/tf_model.h5\n",
      "06:16:17 DEBUG:Creating converter from 3 to 5\n",
      "06:16:33 WARNING:All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "06:16:33 WARNING:All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at output/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# setup imports to use the model\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd5e4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Hello by saying there is one missing link in your statement that I made so let me know. Please do not send my email to others with the same\n",
      "\n",
      "2: Hello it's working fine. I got to check if everything is fine before I start streaming.\"If everything is fine, I know I have an error\n",
      "\n",
      "3: Hello I'm a big fan of POC! It does a really good job of holding my card, and keeps my cash in-bank at the\n",
      "\n",
      "4: Hello) I don't know why we don't know the number, so I think it's there. What does it mean to have that number?\n",
      "\n",
      "5: Hello I want to add the option to enable the automatic reset of the lock-up history. How does that work?\n",
      "\n",
      "\n",
      "I want to disable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello\", return_tensors='tf')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length=30,  \n",
    "    num_return_sequences=5,\n",
    "    #no_repeat_ngram_size=2,\n",
    "    #repetition_penalty=1.5,\n",
    "    #top_p=0.92,\n",
    "    #temperature=.85,\n",
    "    do_sample=True,\n",
    "    #top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i + 1,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bad48000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:37:21 WARNING:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Hello? If so, why is it still listed? Where is this link? Is anything else wrong?\n",
      "\n",
      "2: Hello. How do you plan to use this update if I lose my stuff?  Do you want me to continue doing this update?  This would\n",
      "\n",
      "3: Hello for this, my husband and I've been having problems in the process of getting a new car. I can't get some stuff delivered to me\n",
      "\n",
      "4: Hello. I have a card? Please open the account!\n",
      "\n",
      "My card doesn't work? Please open it. I'm not happy with my\n",
      "\n",
      "5: Hello from Austria I'm at the place and I want to see if there's anyone here. I just saw that people are asking about the hotel that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello\", return_tensors='tf')\n",
    "\n",
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length=30,  \n",
    "    num_return_sequences=5,\n",
    "    #no_repeat_ngram_size=2,\n",
    "    #repetition_penalty=1.5,\n",
    "    #top_p=0.92,\n",
    "    #temperature=.85,\n",
    "    do_sample=True,\n",
    "    #top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i + 1,tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
